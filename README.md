# README - ğŸ“Œ DescripciÃ³n del Proyecto

Este proyecto desarrolla un pipeline automatizado para la ingesta, limpieza, procesamiento y visualizaciÃ³n de datos de producciÃ³n petrolera, utilizando herramientas como Apache Airflow, Knime, Elasticsearch y Kibana. La implementaciÃ³n mejora la eficiencia en el manejo de cuatro bases de datos que se actualizan diariamente, optimizando la toma de decisiones en tiempo real.

ğŸŸ¢ FASE 1: PreparaciÃ³n del Entorno y ConfiguraciÃ³n de Herramientas

ğŸ”¹ Instalar y Configurar Herramientas:

âœ… Knime â†’ Para limpieza y transformaciÃ³n de datos histÃ³ricos.

âœ… Apache Airflow â†’ Para la automatizaciÃ³n de flujos batch y en tiempo real.

âœ… Elasticsearch + Kibana â†’ Para el almacenamiento y visualizaciÃ³n de datos.

âœ… Docker â†’ Para contenerizaciÃ³n y despliegue escalable.

âš ï¸ VerificaciÃ³n: Confirmar que cada herramienta funciona correctamente antes de continuar.

ğŸŸ¢ FASE 2: Procesamiento de Datos HistÃ³ricos en Knime (Batch)

ğŸ”¹ Carga y Limpieza de Datos en Knime:

ğŸ“Œ Datos de Entrada:

1ï¸âƒ£ Coordenadas de Pozos

2ï¸âƒ£ HistÃ³rico de Actividades

3ï¸âƒ£ HistÃ³rico de Presiones

âœ… Pasos en Knime:

Leer los archivos Excel y CSV en Knime.

Realizar limpieza de datos (valores nulos, formatos errÃ³neos, normalizaciÃ³n de datos).

Transformar datos si es necesario (unificaciÃ³n de columnas, cambios de tipo de datos).

Exportar los datos limpios a Elasticsearch.ğŸ“Œ VerificaciÃ³n:

Confirma que los datos estÃ¡n bien estructurados antes de enviarlos a Elasticsearch.

ğŸŸ¢ FASE 3: CreaciÃ³n del Flujo en Tiempo Real con Apache Airflow (Streaming)

ğŸ”¹ AutomatizaciÃ³n con DAGs en Airflow:

âœ… DAG-1: Generar Datos SintÃ©ticos de ProducciÃ³n

Se ejecuta cada  minuto.

Genera datos sintÃ©ticos de producciÃ³n basados en registros histÃ³ricos.

Guarda los datos en un archivo CSV.

âœ… DAG-2: Enviar Datos a Elasticsearch

Se ejecuta despuÃ©s de DAG-1.

Lee el CSV y envÃ­a los datos a Elasticsearch.

Kibana actualiza automÃ¡ticamente la informaciÃ³n.

ğŸ“Œ VerificaciÃ³n:

Ejecutar manualmente los DAGs en Airflow y verificar que los datos aparecen en Kibana.

ğŸŸ¢ FASE 4: VisualizaciÃ³n en Kibana

ğŸ”¹ CreaciÃ³n de Dashboards en Kibana:

âœ… GrÃ¡fica de tendencias de presiÃ³n y producciÃ³n.

âœ… Monitor de producciÃ³n en tiempo real (auto-refresh cada  min).

ğŸ“Œ VerificaciÃ³n:

Asegurar que los grÃ¡ficos en Kibana reflejan correctamente los datos enviados.

ğŸŸ¢ FASE 5: Pruebas Finales y Ajustes

ğŸ”¹ ValidaciÃ³n del Pipeline Completo:
âœ… Ejecutar Knime â†’ Verificar que los datos histÃ³ricos aparecen en Kibana.

âœ… Ejecutar DAGs en Airflow â†’ Confirmar que los datos sintÃ©ticos llegan a Elasticsearch.

âœ… Ajustar errores o mejoras si es necesario.

ğŸ“Œ VerificaciÃ³n:

Confirma que cada componente del pipeline funciona sin errores antes de la implementaciÃ³n final.

ğŸŸ¢ FASE 6: PresentaciÃ³n del Proyecto

ğŸ”¹ Preparar DocumentaciÃ³n y ExposiciÃ³n:

âœ… Crear un diagrama de arquitectura con la descripciÃ³n del sistema.

âœ… Explicar cÃ³mo funciona cada componente del pipeline.

âœ… Mostrar en Kibana los dashboards en vivo con datos en tiempo real.

âœ… Subir cÃ³digo a GitHub con todos los scripts y configuraciones.

ğŸ“Œ RESUMEN DEL FLUJO

1ï¸âƒ£ Instalar y configurar herramientas (Knime, Airflow, Kibana, Elasticsearch).

2ï¸âƒ£ Cargar y limpiar datos histÃ³ricos en Knime â†’ Enviar a Kibana.

3ï¸âƒ£ Crear DAGs en Airflow para generar y enviar datos sintÃ©ticos en tiempo real.

4ï¸âƒ£ DiseÃ±ar dashboards en Kibana para visualizar datos histÃ³ricos y en tiempo real.

5ï¸âƒ£ Probar y ajustar el pipeline completo.

